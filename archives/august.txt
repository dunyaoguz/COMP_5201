From probst at cse.concordia.ca  Mon Aug 10 17:30:45 2020
From: probst at cse.concordia.ca (David K. Probst)
Date: Mon, 10 Aug 2020 17:30:45 -0400
Subject: [comp5201-f20] lecture 3
Message-ID: <5f31bc85.7dnOTCJ1nS4SeqSE%probst@cse.concordia.ca>

 
Lecture 3  Computer Organization and Design                    DKP/Fall 2020
_________

Character Sets
______________

We discussed how to use bit patterns to represent natural numbers ("unsigned
numbers") and integers that might be negative ("signed numbers").  Let's
briefly review how to represent characters.

The ASCII system is a slightly old-fashioned way to represent characters.
Each character fits into a byte.  Normally, 8 bits would give us 2^8 = 256
characters, but ASCII only uses the lower-order 7 bits to distinguish
characters.  ASCII is thus able to represent 128 different characters.

The character set is everything you can type on an American standard keyboard
plus some formatting characters.  Germans who can't live without their
umlauts can always use Unicode, which requires two bytes per character.
Humans typically use tables when they try to figure out which bit pattern
corresponds to which character.

Floating-Point Numbers
______________________

All numbers represented inside computers are _binary rationals_ (definition
follows).  This, and the finite size of computers, has consequences for the
accuracy with which we can represent arbitrary rational numbers, and, a
fortiori, arbitrary real numbers.

Computers can't represent all binary rationals, because of finite register
size, but they can represent a bit-limited subset of the binary rationals.
These are often reasonable approximations to rationals and real numbers.
Of course, bit-limited bit patterns can _exactly_ represent a finite subset
of the binary rationals.  Definition: A binary rational is a rational whose
denominator is a power of 2.

Schemes that use bit-limited binary rationals to approximate fractional
numbers introduce a trade-off between _precision_ and _range_.  We get
precision when the binary rationals are close together; we get range when
the difference between the smallest and largest binary rational is large.

A number 'n' has a finite binary expansion iff 'n' is a binary rational.

A reduced binary rational fraction is a number of the form m/2^n, where
m < 2^n and the radix 2 is not a factor of 'm'.  The binary expansion of
such a number has exactly 'n' binary places to the right of the binary
point.  This identity is actually true for arbitrary radix.

It helps to spend a moment on fixed-point numbers before moving on to
floating-point numbers.  It also helps to have a sensible presentation
strategy.  Our strategy is to develop a consistent, intuitive _blackboard
notation_ for floating-point numbers, and only then worry about how
blackboard notation can be adapted to correspond to a register with a fixed
number of bits.

The reason for caring about fixed-point numbers is that every floating-point
number is a scaled version of a (1+f)-bit binary fixed-point number, where
'f' is the number of bits set aside for the fractional part of the significand.

A fixed-point number consists of an _integral part_ and a _fractional part_,
with the two parts separated by a _radix point_.  If a radix-r fixed-point
number has 'm' "integer" digits and 'n' "fractional" digits, its value is:

    m-1
x = Sigma x_i * r^i = (x_m-1 ... x_0 <point> x_-1 ... x_-n)_r
    i=-n

The digits to the right of the radix point are given negative indices and
their weights are negative powers of the radix.

In an (m+n)-digit radix-r fixed-point number system with 'm' whole digits,
numbers from 0 to r^m - r^-n, in increments of r^-n, can be represented.
We call r^-n the _step size, or _resolution_.  We will focus on radix 2.
In this case, any two adjacent (m+n)-bit fixed-point binary rationals have
a fixed distance between them, viz., 2^-n.   

For example, in a (2+3)-bit binary fixed-point number system, decimal 2.375
= (1 * 2^1) + (0 * 2^0) + (0 * 2^-1) + (1 * 2^-2) + (1 * 2^-3) = (10.011).
(I won't write the radix specifier "_2"; it is understood).  In this number
system, the values 0 = (00.000) through 2^2 - 2^-3 = 3.875 = (11.111) are
representable in steps of 2^-3 = 0.125.  For a fixed sum (m+n), there is a
trade-off.  A larger 'm' leads to an enlarged _range_ of numbers.  A larger
'n' leads to increased _precision_ in specifying numbers, which, for us,
means better approximations to real numbers.

There are standard procedures to convert between decimal and binary
fixed-point, which are somewhat mechanical.  However, they do make bite-size
homework problems, so here are two examples.

Example: Convert decimal 2.9 to (3+5)-bit binary fixed point.

Integer 2 is handled separately: (010.???).

Now, .9 * 2 = .8  1
     .8 * 2 = .6  1
     .6 * 2 = .2  1
     .2 * 2 = .4  0
     .4 * 2 = .8  0
     --------------
     .8 * 2 = .6  1

Just taking the first five fractional digits gives (010.11100) = 2.875.
But we _could_ do something with the sixth digit: since it is 1, we can
add 1 to the current approximation.  This gives (010.11101) = 2.90625,
which is closer to 2.9 than is (010.11100).  This last refinement is called
_rounding_.  We won't use rounding even if it often improves accuracy.  

Example: Convert (1101.0101) to decimal.

This is obviously 13 and 5/16, which is 13.3125.  The binary rational is
213/16.

Although there are several ways to encode signed fixed-point numbers, we
will choose just one.  In fixed-point notation, we will just write the
sign explicitly.  Thus, -5.75 in (3+5)-bit binary fixed point is just
-(101.11000).

We need some terminology.  When we get to floating-point numbers, we will
see that they are logically represented as: +/- s * 2^e.  The three components
of the representation are: 1) the _sign_, 2) the _significand_ 's', and 3) the
_exponent_ 'e'.  That is, plus or minus some binary rational times 2 raised to
some positive or negative (integer) power.

This is not as foreign as it looks.  In the decimal world, we often write,
say, -3.45 * 10^7, where 3.45 is a (decimal) rational in the range [1,10).

Note: The phrase "decimal rational" is ambiguous: it might be a rational
number represented in decimal, or it might be an analogue of "binary rational".

We can use "blackboard notation" as a stepping stone to representing
floating-point numbers in registers.  Blackboard notation is easier because
i) we use trivial representations of sign and exponent, and ii) we use
examples with terminating significands, and---for nonterminating ones---make
use of repeating binary expansions of infinite length.

Let's try some examples in blackboard notation.  Recall that, in scientific
notation, we usually write a single digit to the left of the decimal point,
e.g., 3.26 * 10^5.  We adopt the same convention for _binary rationals_,
e.g., 1.01 * 2^-2 = 0.3125.  That is, we write a positive binary rational
with a single nonzero digit to the left of the radix point, and continue
with the fractional part, all this times some (positive or negative) power
of two.

Example: Convert (1101.1010) to normalized blackboard floating point.

(1101.1010) = (1101.1010) * 2^0  <identity>
            = (1.1011010) * 2^3  <normalize>
            = (1.1011010) [3]    <just being lazy>

Example: Convert (0.000111) to normalized blackboard floating point.

(0.000111)  = (0.000111) * 2^0   <identity>
            = (1.110000) * 2^-4  <normalize>
            = (1.110000) [-4]    <just being lazy>

When we move the binary point left, we increase the exponent; when we move
the binary point right, we decrease the exponent.

This is almost as far as we can take our blackboard notation.  To show
actual bit patterns, we need to know how many bits are available for the
significand, and how many bits are available for the exponent.  Also, in
standard-computer bit patterns, we will drop the (thus far explicit) "1."
in that it goes without saying.  This allows us to explicitly represent
only (a prefix of) the fractional part of the significand.

Note: A simpler word than "significand" is "coefficient".  I will use
"coefficient". 

Consider a 16-bit register.  One bit is used to represent the sign, leaving
us with 15 bits.  After reflection, we choose to use 4 bits (one hex digit)
to represent the signed exponent in two's complement semantics.  That leaves
only 11 bits to store the _fractional part_ of the significand.

Example: Represent 5/16 as a floating-point number in a 16-bit register.

I will use the order: sign, exponent, fractional part.  Now, 5/16 is
1.01 * 2^-2.  The 4-bit exponent in two's complement is 1110 (hex e).
So the full 16 bits are: 0 | 1110 | 01000000000, which is 7200 in hex.

To sum up, we use: i) one bit for the sign, ii) 4 bits to represent the
exponent in two's complement, and iii) 11 bits for the bits to the _right_
of the binary point in the full significand.  (Again, these "right-of-point"
bits are called the _fractional part_ of the significand).

Example: Put (1.1010101) [-3] into a 16-bit register.

This is: 0 | 1101 | 10101010000, which is 6d50 in hex.

Example: Put (1.11) [4] into a 16-bit register.

This is: 0 | 0100 | 11000000000, which is 2600 in hex.

Example: Put 1/5 into a 16-bit register.  Here, the true value of the
fractional part of the binary-rational coefficient is an infinite binary
expansion.

0.2 = (0.<0011>*) = (1.<1001>*) [-3]  This is still blackboard notation.

This becomes: 0 | 1101 | 10011001100, which is 6ccc in hex.

But any notion of fixed-point has vanished from the last example.  So we
have another possible notation.  Pretending we are mathematicians, we could
 write:

0.2 = 0.(0011)*

This is just the infinite binary expansion of 0.2 without normalization,
and without scale factor.

Other people have given this matter much thought.  In the current IEEE
floating-point standard, a floating-point number has three components:
i) a sign +/-, ii) a significand 's', and iii) an exponent 'e'.  The
_exponent_ is a signed integer represented in biased format (a fixed
bias is added to it to make it into an unsigned number).  We are _not_
responsible for exponent bias.  I mention it only for completeness.
The _significand_ is a fixed-point number in the range [1,2).  Because
the binary representation of the significand always starts with "1.",
this fixed "1." is omitted ("hidden"), and only (a prefix of) the
fractional part of the significand is explicitly represented.

What is being represented in this way?  Answer: +/- s * 2^e.

There are short (32-bit) and long (64-bit) floating-point formats.  The short
format ranges from 1.2 * 10^-38 to 3.4 * 10^38.  The long format ranges from
2.2 * 10^-308 to 1.8 * 10^308.

Exercise: Calculate the four bounds for strictly positive single- and
double-precision floating-point binary rationals.  Single: The smallest
strictly positive binary rational is 2^-126, which is (approximately)
1.2 * 10^-38; the largest binary rational is (approximately) 2^128, which
is (approximately) 3.4 * 10^38.  Double: The smallest strictly positive
binary rational is 2^-1022, which is (approximately) 2.2 * 10^-308; the
largest binary rational is (approximately) 2^1024, which is (approximately)
1.8 * 10^308.  Zero lies outside either range.

Note: 1.2 is more precisely 1.17549435... .  The others are similar. 

The distance between two adjacent (m+n)-bit floating-point binary rationals
varies considerably.  Consider single-precision binary rationals, which we
may characterize as (8+23)-bit floating-point binary rationals.  When the
scale factor is near 1, the gap is near 2^-23.  Consider the smallest
strictly positive floating-point binary rational, which is 2^-126.  The
distance to the next largest binary rational is 2^-23 * 2^-126 = 2^-149.
Consider the largest floating-point binary rational, which is roughly 2^128.
The distance to the next smallest binary rational is 2^-23 * 2^127 = 2^104.
The gap between adjacent floating-point binary rationals is a function of
their position on the real line.

Let's say a few more words about the current IEEE standard.  If a word has
32 bits, and there are 8 exponent bits, then the significand has 23 bits
(plus 1 hidden), the significand range is [1,2 - 2^-23] (or [1,2), if you
prefer), and the exponent bias is 127.  There are also bit patterns for 0,
Infinity, and Not-a-Number (NaN).  Again, we are not responsible for this.

I should add that there is no free lunch, and that the two most negative
exponents in the exponent range are _removed_ to give us bit patterns
that are used in special representations of special values.

Let's see how a floating-point number would be laid out in a 32-bit word.
Bit 31 would be the sign bit of the binary rational.  Then, bits 30 through
23 (8 bits) would store the 8-bit exponent field (including the sign of the
exponent), while bits 22 through 0 (23 bits) would store a 23-bit binary
rational in the range [0,1 - 2^-23].  (I oversimplify slightly; the value 0
must be handled separately).

To summarize the current IEEE standard for floating-point numbers, in the
short (32-bit) format, we have the sign bit, 8 bits for the exponent, and
23 bits for the fractional part of the significand (the hardware adds the
implicit hidden "1."), while in the long (64-bit format), we have the sign
bit, 11 bits for the exponent, and 52 bits for the fractional part of the
significand (the hardware adds the implicit hidden "1."). 

Most computers offer _double-precision_ floating point.  As we have just
seen, we increase the exponent field from 8 bits to 11 bits, and the fraction
field from 23 bits to 52 bits.  Although double precision does increase the
exponent range, its primary advantage is in its greater precision, which
leads to greater accuracy (closer approximation of reals by binary rationals).

Note: In both 32-bit and 64-bit machines, single-precision floating-point
arithmetic means use of 32-bit registers, and double-precision floating-point
arithmetic means use of 64-bit registers.

Instruction Formats
___________________

Although instruction formats have consequences in terms of the ease with
which certain operations can be carried out, and whose simplicity and
uniformity is absolutely critical to the speed with which a sequence of
machine instructions can be _pipelined_ efficiently, they are not in
themselves very interesting.

Having briefly reviewed the encoding structures for different types of
numbers, let us now consider _instruction encoding_.

In one computer, a typical machine instruction is 'add r1,r2,r3', which
causes the values in 'r2' and 'r3' to be added, and the sum put into 'r1'.

A machine instruction for an arithmetic/logic operation specifies an opcode,
one or more source operands, and, usually, one destination register.  The
opcode is a binary code (bit pattern) that specifies an operation.  The
operands of an arithmetic or logical instruction can come from a variety of
sources.  The method used to specify where the operands are to be found, and
where the result must go, is called the _addressing mode_, or _addressing
scheme_.  For now, we assume that all operands are in registers, and discuss
other addressing modes gradually.

In the computer mentioned, there are three instruction formats.

1) Register or R-type instructions operate on the two registers identified
in the 'rs' and 'rt' fields, and store the result in register 'rd'.

R: opcode  rs      rt      rd      <other stuff>

   6 bits  5 bits  5 bits  5 bits  11 bits  = 32 bits

2) Immediate or I-type instructions come in two flavors.  The general format
for an I-type instruction is:

I: opcode  rs      rt      immediate        (immediate = operand or offset)

   6 bits  5 bits  5 bits  16 bits          = 32 bits

i) In _true immediate_ instructions, the 16-bit immediate field in bits
0 - 15 holds a 16-bit signed integer that plays the same role as 'rt' in
the R-type instructions; in other words, the specified operation is
performed on 'rs' and the immediate operand, and the result is written
into 'rt', which is now a destination register.

Example: 'daddiu r1,r1,#-8' lays out as

   [daddiu]  [r1]    [r1]    [-8]

   6 bits    5 bits  5 bits  16 bits

We add the 16-bit immediate (here, -8) to 'r1' to compute a number (often
a memory address).  Then, we write this number into 'r1'.

ii) In _load, store, and branch_ instructions, the 16-bit field is
interpreted as an _offset_, or relative address, that is to be added to
the _base_ value in register 'rs' (resp., the program counter) to obtain
a memory address for reading or writing memory (resp., transfer of control).

For _data accesses_, the offset is the number of _bytes_ forward (positive)
or backward (negative) relative to the base address.  In contrast, for
_branch instructions_, the offset is in _words_, given that instructions
always occupy complete 32-bit memory words.  To interpret the 16-bit signed
integer as a word-address offset, we must multiply by 4 to get the number of
bytes.  Since offsets can be positive or negative, this allows for branching
to other instructions within +/- 2^15 (32,768) instructions of the current
instruction.

We describe two _data-transfer_ instructions, and a _branch instruction_,
separately.

D: opcode  rs      rt      immediate         -- data transfer

   6 bits  5 bits  5 bits  16 bits

Example: 'l.d f6,-24(r2)' lays out as

   [l.d]   [r2]    [f6]    [-24]

   6 bits  5 bits  5 bits  16 bits

We add the immediate byte-offset -24 to 'r2' to determine a memory address.
Then, we load the double-precision floating point number (64 bits) from
that memory location and put it into floating-point register 'f6'.

Example: 's.d f6,24(r2)' lays out as

   [s.d]   [r2]    [f6]    [24]

   6 bits  5 bits  5 bits  16 bits

We add the immediate byte-offset 24 to 'r2' to determine a memory address.
Then, we store the double-precision floating point number (64 bits) in
floating-point register 'f6' into that memory location.

B: opcode  rs      rt      immediate         -- conditional branch

   6 bits  5 bits  5 bits  16 bits

Example: 'bne r1,r2,loop' lays out as

   [bne]   [r1]    [r2]    [loop]

   6 bits  5 bits  5 bits  16 bits

We compare register 'r1' and register 'r2'.  If they are not equal, we add
the word-offset derived from the immediate 'loop' to the current value of
PC as the new value of PC.  That is, we add the shifted, sign-extended
16-bit signed integer 'loop' to the memory address of the branch instruction.
We have already seen how far we can go from the current instruction.

3) Jump or J-type instructions cause unconditional transfer of control to
the instruction at the specified address.  Since only 26 bits are available
in the address field of a J-type instruction, two conventions are used.
First, as the 26-bit field is assumed to specify a word address (as opposed
to a byte address), two zero bits are appended to the right.  We now have
28 bits.  The four missing bits are stolen from PC, as will be described
shortly.

J: opcode  partial jump-target address       -- unconditional branch

   6 bits  26 bits                          = 32 bits

Example: 'j done' lays out as

   [j]     [done]

   6 bits  26 bits

Using our two tricks, we expand the partial jump-target address 'done'
into a full 32-bit address, and transfer control there.

Addressing modes
________________

Addressing mode is the method by which the location of an operand is
specified within an instruction.  Our computer uses five addressing
modes, which are described as follows:

1. Immediate addressing: The operand is given in the instruction itself.
A simple example is 'daddi'.  A second example is shown in its full glory:

daddui r1,r1,#-8  <subi r1,r1,8 in RISC-V>

Because of the 'u', this is a "natural-number" add, which makes sense,
for example, when computing memory addresses.  After all, addresses are
themselves natural numbers, and overflow is reasonably unlikely here.
Our odd-looking instruction takes nonnegative 'r1', adds the immediate
-8 to it, but does not bother to check for overflow.  Of course, if
'r1 = 7', this is a problem.

In truth, error detection is done at a higher level in the hardware/software
stack.  Still, it is prudent to exercise great caution in programming in this
manner.

But suppose we are not computing memory addresses, and write:

daddi r1,r1,#-8

Again, the second operand is given in the instruction itself.  The first
operand is 'r1'.  The 16-bits holding -8 are the second operand (or
actually the lower half of it).  However, with this opcode, we must most
definitely check for overflow.  After all, 'r1' might be a large, negative
number.

2. Register addressing: The operand is taken from, or the result placed
into, a specified register.  Here is an example with two source registers
and one destination register:

mul.d f4,f2,f6

The contents of registers 'f2' and 'f6' are read.  A double-precision
floating-point multiply takes place, and the result is placed into
register 'f4'.

3. Base addressing: The operand is in memory and its location is computed
by adding a byte-address offset (16-bit signed integer) to the contents of
a specified base register.  Examples:

l.d f6,-24(r2)  ; f6 is a destination register

s.d f6,24(r2)   ; f6 is an operand register

4. PC-relative addressing: This is the same as base addressing, except that
the "base" register is always PC, and a hardware trick is used to extend the
signed-integer offset to 18 bits.  Namely, we multiply by 4 to obtain a
word-address offset, which is _subsequently_ sign extended to 32 bits.  This
addressing mode is used in conditional branches.  Example:

beq r1,r2,found

Again, the 16-bit signed number is multiplied by 4 (making it a word-address
offset) and the result is added to PC.  This allows branching to other
instructions within +/- 2^15 words of the current instruction. 

5. Absolute addressing: The addressing mode for unconditional branches is
different because we don't really have a "base" register.  Example:

j done

Here, we need fancier hardware tricks.  'done' is a 26-bit natural number.
Multiplying by 4 gives us a 28-bit natural number.  Now, if we pad the front
of 'done' with the four leading bits of PC, we obtain a genuine 32-bit (word)
address.  Thus, we can "goto" instructions much further away than those within
+/- 2^15 words of the current instruction.

Special Values in IEEE Floating Point (we are not responsible for this);
_____________________________________

I have told you that the exponent field is in two's complement.  Suppose we
have a 4-bit exponent field.  Then:

bit pattern     true exponent
___________     _____________

0000             0
0001             1
...             ...
0111             7
1000            -8
1001            -7
1010            -6
...             ...
1111            -1

I don't teach the five special values (+/- 0, +/- infinity, and NaN), but I
agree that they are necessary in real computers.  To represent these values,
we need special codes.  But this means will we have to sacrifice certain bit
patterns that we could otherwise have used.

4-bit two's complement runs from -8 to 7.  That's 16 possible exponent values.
The IEEE standard steals two exponent values---the two smallest values,
leaving you with only 14.  Here are the fourteen that remain:

bit pattern     true exponent
___________     _____________

0000             0
0001             1
...             ...
0111             7
*******************
1000            -8 <stolen to participate in special codes>
1001            -7 <stolen to participate in special codes>
*******************
1010            -6
...             ...
1111            -1

We are left with an exponent range of -6 to 7, as I show in my lectures.  Why
might a student care?  If someone asks you, what is the minimum normalized
IEEE floating-point number in 16 bits, with a sign bit and 4 exponent bits,
you need to know whether that minimum magnitude is 1.<0:11> * 2^-6  or
1.<0:11> * 2^-8

I generally ask for maximum magnitudes, so this problem never arises, but if
I were to ask for minimum magnitudes, I would tell you whether to use -6 or -8.

Stealing is real; bias is just a convenience.  Amateurs usually mix stealing
and bias so awkwardly---incompetently, I would say---that the student loses
the big picture.

Why is bias convenient?  Well, the normal range is -8 to 7.  Take the largest
number (i.e., 7), and add it to each of the two stolen exponents.  (We also
add 7 to the exponent bit patterns that have not been stolen; this does _not_
change their true values, only their representations).

   1000   -8 <stolen to participate in special codes>
  +0111
   ____
   1111   <no longer an exponent; now a special code part>

   1001   -7 <stolen to participate in special codes>
  +0111
   ____
 1|0000   <no longer an exponent; now a special code part>

That is why 32-bit normalized IEEE floating-point numbers have magnitudes that
run from 2^-126 to (approximately) 2^128 (in decimal, 1.2 * 10^38 to
3.4 * 10^38---roughly).  In contrast, 64-bit normalized IEEE floating-point
numbers have magnitudes that run from 2^-1022 to (approximately) 2^1024 (in
decimal, 2.2 * 10^-308 to 1.8 * 10^308---roughly).  See Lecture 3.

However, you are _still_ not responsible for special values, special
encodings, exponent stealing, or exponent bias.  This appendix is for
information only.  Disclosure: I may talk about exponent stealing.




From probst at cse.concordia.ca  Tue Aug 11 17:32:02 2020
From: probst at cse.concordia.ca (David K. Probst)
Date: Tue, 11 Aug 2020 17:32:02 -0400
Subject: [comp5201-f20] lecture 4
Message-ID: <5f330e52.t2xOzF2AqSvmr4pH%probst@cse.concordia.ca>


Lecture 4  Computer Organization and Design                    DKP/Fall 2019
_________

Digital Logic
_____________

This is a conundrum.  An introductory course on computer organization and
design should focus on concepts rather than on engineering detail (I
assume students are not focused on hardware design), and should explain the
subject from a programmer's point of view, and emphasize consequences for
programmers.  Normally, I would omit logic design altogether.  But perhaps
I can give you a taste of it without getting bogged down in details.

Logic operators are abstractions for specifying transformations of binary
signals.  The simplest logic circuits are _combinational_.  This means that
they do not operate with _memory_, or _state_.  Combinational circuits may
be abstracted as Boolean functions, which are familiar to us from our study
of _sentential logic_ (also called _propositional calculus_).

The mathematical concept underlying combinational logic is that of _n-place
Boolean function_.  A Boolean function specifies what operation we want a
combinational circuit to compute.  In all cases, there are many 
implementations of a given Boolean function.  To speak of this, we need
terminology to specify Boolean functions, and means to describe particular
implementations.  Diagrams will describe implementations, and formulas
from sentential logic will specify Boolean functions.  

Physically, combinational circuits are constructed by wiring together some
number of logic gates, which have names like 'NOT', 'AND', 'OR', 'XOR',
'NAND', 'NOR', and 'XNOR'.  The most familiar gates correspond to familiar
binary sentential connectives.  Other gates correspond to slightly less
familiar binary sentential connectives (e.g., '+', "downward arrow", and
'|').  Of course, the most familiar sentential connectives are binary,
while logic gates may have more than two inputs (and, to a lesser extent,
more than one output).  Multiple-input 'NAND', 'NOR', and 'XNOR' gates are
_defined_ as the negations of multiple-input 'AND', 'OR', and 'XOR' gates,
respectively.  I will tell you the Boolean function that multiple-input
'XOR' gates compute in a moment.

An n-place _Boolean function_ is a map from {T, F}^n to {T, F}.  For each
'n', there are 2^(2^n) n-place Boolean functions.  It is convenient to
identify a connective with the Boolean function it computes.  This gives
us 2^(2^n) n-ary connectives.  We will look at all connectives with n <= 2. 

There are two 0-place Boolean functions, 'T' and 'F'.  The corresponding
connectives---which are not representable as ASCII characters---are 'T' and
'inv. T'.  There are four unary connectives, but only negation is of any
interest.  There are sixteen binary connectives, but only the last ten listed
below are "really binary".  I provide a correspondance table, which can be
used for reference.  I list all sixteen two-place Boolean functions and the
connectives that correspond to them.

Digression: Are these sixteen binary connectives are sufficient to compute
_all_ Boolean functions?  Would it help to add, say, the ternary majority
connective '#', which is true whenever a majority of its three arguments is
true?  No.  We can compute everything we need from {'~', '/\'} or {'~', '\/'},
to give just two of several examples.  Still, logicians enjoy naming some
interesting (if not strictly necessary) ternary---and higher---connectives.

logic    name                  gate   meaning                 multiple input
_____    ____                  ____   _______                 ______________
        
T        true                         nullary; always true

inv. T   false                        nullary; always false
 
p        --                    --     'p'

q        --                    --     'q'

~p       not                   NOT    not 'p'

~q       not                   NOT    not 'q'

p /\ q   and                   AND    'p' and 'q'             "all"

p \/ q   or                    OR     'p' or 'q'              "any"

p --> q  conditional           --     if 'p', then 'q'

p <--> q biconditional         XNOR   'p' iff 'q'             "not odd"

p <-- q  reversed conditional  --     if 'q', then 'p'

p + q    exclusive or          XOR    'p' or 'q', but         "odd"
                                      not both 
  |
p v q    nor                   NOR    neither 'p' nor 'q'     "not any"

p | q    nand                  NAND   not both 'p' and 'q'    "not all"

p < q    --                    --     (not 'p') and 'q'

p > q    --                    --     'p' and (not 'q')

Remark: None of {'-->', '<--', '<', '>'} is associative or commutative.

We can pair Boolean functions/connectives with their negations:

                                                               |
<T, inv. T>;  <p, ~p>;  <q, ~q>;  <p /\ q, p | q>;  <p \/ q, p v q>;

<p + q, p <--> q>;  <p --> q, p > q>;  <p <-- q, p < q>

In other terminology: <NOT, ident>; <AND, NAND>; <OR, NOR>; <XOR, XNOR>

Example: An electrical device with three inputs (clearly combinational).

         +--------+
 x1 ---> |        |
 x2 ---> |        | ---> f(x1,x2,x3)
 x3 ---> |        |
         +--------+

To say that the device has no memory is to say that the present output level
depends only on the present input levels (and not on the past history of the
device).

Consider the two-input 'AND' gate:

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q
  q ---> |        |
         +--------+

We can label the output with a logical formula.

Consider the two-input 'OR' gate:

         +--------+
  p ---> |        |
         |   OR   | ---> p \/ q
  q ---> |        |
         +--------+

Now, consider the less familiar two-input 'NAND' gate:

         +--------+
  p ---> |        |
         |  NAND  | ---> p | q
  q ---> |        |
         +--------+

Finally, consider the unfamiliar two-input 'XNOR' gate:

         +--------+
  p ---> |        |
         |  XNOR  | ---> p <--> q
  q ---> |        |
         +--------+

You do not have to know all the logic-gate names or all the logic connectives;
they will be provided to you as needed.

Here is the biggest circuit I am willing to draw today:

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q ---+
  q ---> |        |                |
         +--------+                |     +--------+
                                   +---> |        |
                                         |   OR   | ---> (p /\ q) \/ ~r
                                   +---> |        |
         +--------+                |     +--------+
         |        |                |
  r ---> |  NOT   | ---> ~r -------+  
         |        |
         +--------+

It should be intuitive that, just as every logical formula is tautologically
equivalent to many other quite distinct logical formulas, so the same Boolean
function can be implemented by many different circuits (i.e., different
combinations of gates).  For example, each fully parenthesized Boolean
expression (wff) corresponds to a _unique_ tree of sentence symbols and
connectives, and thus suggests a particular implementation of the Boolean
function.

Whereas the process of converting a logic expression to a logic diagram, and
then to an associated hardware realization, is trivial, obtaining a logic
expression that leads to the best possible hardware circuit is not.  For one
thing, the definition of "best" changes depending on the technology and the
implementation scheme being used (e.g., custom VLSI, programmable logic, or
discrete gates), and on the design goals (e.g., area, time, power, and cost).
For another, the simplification process, if not done via automatic design
tools, is not only cumbersome but also imperfect; for example, it might be
based on minimizing the number of gates employed, without taking into account
the time and energy implications of the wires that connect the gates.

I will not teach you any hand-minimization techniques for logic circuits,
which I regard as obsolete.

Today, all digital design of processors and related hardware systems is done
using a _hardware description language_, along with software tools for
digital analysis and synthesis.  The current specification language is VHDL.

The next step up from combinational logic is _sequential logic_, which we
need to make registers, caches, and memories.  You may think of a sequential
circuit as a combinational circuit plus timed state elements.

The behavior of a combinational (memoryless) circuit depends only on its
current inputs, not on past history.  A sequential circuit, on the other
hand, has a finite amount of memory whose content, determined by past inputs,
affects the current output behavior.

Combinational circuits implement _Boolean functions_.  Sequential circuits
implement _finite-state machines_.  In the simplest case, the machine stores
the current state.  When input is received, the current state is turned into
the next state, and then output.  State plays a big role in pipelines.

A state element has at least two inputs and one output.  The required inputs
are the data value to be written into the element, and the clock, which
determines when the data value is written.  The output from a state element
provides the value that was written in an earlier clock cycle.  The clock is
used to determine when the state element should be written; a state element
can be read at any time after it has been written.  Note: there exist state
elements without clocks, but we ignore them.

Because only state elements can store a data value, any collection of
combinational logic must have its inputs come from a set of state elements,
and its outputs written into a set of state elements.  Below, 'd' = data.

Example:                        _____________
          +-----------+        /             \        +-----------+
      d   |           |  d    /               \   d   |           |  d
     ---> | state     | ---> /  combinational  \ ---> | state     | --->
          | element 1 |      \  logic          /      | element 2 |
          |           |       \               /       |           |
          +-----------+        \_____________/        +-----------+
                ^                                           ^
                |                                           |
              clock                                       clock

When a clock signal is received, the value of the data input to a state
element is instantaneously stored in the state element.  This value is
stable until the next clock signal is received.

The state elements provide valid inputs to the combinational logic block.
But the combinational logic itself is not instantaneous; rather, it needs
time to settle.  To ensure that the values written into the state element
on the right are valid, the clock must have a long enough period so that 
all the signals in the combinational logic block stabilize, _after_ which
the stable values can be stored into the receiving state element.

Finally, I will give a very brief sketch of how combinational logic may be
used to implement a very simple (and slow) adder.

When two bits are added, the sum is a value in the range [0,2] that can be
represented by a _sum bit_ and a _carry bit_.  A circuit to compute both is
known as a _half adder_.  Let me use the symbol '+' to represent 'XOR'
(exclusive or).  That is, 'p + q' is equivalent to '(p \/ q) /\ ~(p /\ q)'.
A moment's reflection shows that a half adder can be implemented with an
'AND' gate and an 'XOR' gate, since sum = 'p + q' and carry = 'p /\ q'.

Here is a (possibly unnecessary) illustration of a half adder:

              +--------+
  p --o-----> |        |
      |       |  AND   | ---> p /\ q = carry
  q ----o---> |        |
      | |     +--------+
      | |
      | |     +--------+
      +-----> |        |
        |     |  XOR   | ---> p + q = sum
        +---> |        |
              +--------+

Two inputs are fed into each of two gates, giving us two outputs.

By adding a carry input to a half adder, we get a _full adder_.

           c_in
             |
             v   
         +--------+
  p ---> |        |
         |   FA   | ---> sum
  q ---> |        |
         +--------+
             |
             v
           c_out

Here is a straightforward implementation of a full adder:

         +--------+
  p ---> |        |
         |   HA   | ---> p /\ q ------+
  q ---> |        |                   |
         +--------+                   |
             |                        |           +--------+
       p + q |           +--------+   +---------> |        |
             +---------> |        |               |   OR   | -------------->
                         |   HA   | ------------> |        | (p /\ q) \/
  c_in = r ------------> |        | (p + q) /\ r  +--------+ ((p + q) /\ r)
                         +--------+                          = c_out
                             |
                             v
                      (p + q) + r = sum

Abstractly, a full adder takes three inputs ('p', 'q', and 'r') and computes
two Boolean functions as indicated.  There are implementations other than 
the one shown, and other ways of naming the two Boolean functions.

Some of the other names are:

  sum   = p + q + r |= =| p <--> q <--> r |= =| (+)^3 pqr.        
  c_out = pq \/ r(p + q) |= =| pq \/ pr \/ qr |= =| #pqr.

'(+)^3' is ternary addition modulo 2.  '#' is the ternary majority connective.

Since the output labels---which both specify the Boolean functions the full
adder computes, and trace the circuit operation---are fully parenthesized,
they indicate how an implementing circuit _was_ synthesized from simpler
Boolean expressions.  Thus, we could work backwards, starting with these
expressions as the specfication, and translating them directly into an
implementing circuit.

Noting the reuse of 'p + q', we get a slightly different picture:

            ^               c           s = p + q + r
            |               ^
            |               |
            |        s      OR          c = (p /\ q) \/
            |        ^     / \              (r /\ (p + q))
       time |        |    /   \
            |        +   AND   AND
            |       / \ / \   / \       s = (+)^3 pqr
            |      r   +   r p   q
            |         / \               
            |        p   q              c = #pqr

A full adder, connected to a state element for holding the carry bit from
one cycle to the next, functions as a _bit-serial_ adder.  A _ripple-carry
adder_, on the other hand, unfolds the sequential behavior of a bit-serial
adder into space, using a cascade of 'k' full adders to add two k-bit
numbers.  There are a number of faster adders.

Once we have an adder, we can implement, say, a _counter_, and so on.

Digital-Logic Review
____________________

A combinational circuit with 'k' inputs and one output is a hardware
realization of a k-place Boolean function.

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q
  q ---> |        |
         +--------+

For example, an 'AND' gate is a hardware realization of a simple two-place
Boolean function.  It is trivial to write out the truth table.  The diagram
is self-explanatory.

The output has been labeled with a logical formula.  It completely defines
the Boolean function realized by this circuit, which here is a single gate.

This logical formula is not unique: '(p /\ q) /\ (p \/ ~p)' represents the
same Boolean function.

For each function, there is at least one formula that expresses it.  Some
are more concise.  We are not responsible for transforming logical formulae
into their "optimal" forms, whatever you might mean by this.  Also, when we
translate formulae into circuits, we are usually given a _vocabulary_ of what
connectives, i.e., what gates, we are allowed to use.  The simplest
vocabulary is: '/\', '\/', '~' ('AND', 'OR', 'NOT').

Suppose someone gives you information about a Boolean function's truth table.
For example, suppose someone says: Three-place Boolean function 'f' is true
for '001' and '010', and false everywhere else.

You can write down immediately: ~p.~q.r + ~p.q.~r  With pen and paper, you
can more elegantly write:
               _ _     _   _
               p q r + p q r

i.e., explicitly list the triples that make this function true.  This logical
formula happens to be in _sum-of-products_ form.

Definition:: sum-of-products form: a Boolean sum of terms, each term being a
Boolean product of variables and their complements.

This involves a change of notation.  Products are simple concatenation,
while sum is indicated by '+'.  And complementation is indicated by writing
a _bar_ over the variable. 

To have one sum-of-products form doesn't mean it's the shortest one.
            _       _ _   _ _                 _   _         
  p q r + p q r + p q r + p q r |= =| p r + p q + q r

We are not responsible for transforming the first formula into the second,
a process often called _optimization_.

We are responsible for translating (reasonably nice) formulae directly into
circuits.
                   _ _
Consider r + p q + p q .  This formula is _not_ fully parenthesized.  We try:

         +--------+
  p ---> |        |
         |  AND   | ---> p /\ q ---+
  q ---> |        |                |
         +--------+                |                 +--------+
                                   +---------------> |        |
                                                     |   OR   | ---> p q +
                                                     |        |      _ _
         +--------+                                  +--------+     (p q + r)
  p ---> |  NOT   | ---+                                 ^
         +--------+    |   +--------+   +--------+       |
                       +---|        |   |        |       |
                           |  AND   |---|   OR   |-------+
                       +---|        |   |        |
         +--------+    |   +--------+   +--------+
  q ---> |  NOT   | ---+                    ^
         +--------+                         |
                                            |
  r ----------------------------------------+

Here, the longest path is: ** 'NOT'; 'AND'; 'OR'; 'OR' **.

But equivalently:

         +--------+      +--------+
  p ---> |        |      |        |
         |  AND   | ---> |   OR   | ---+
  q ---> |        |      |        |    |
         +--------+      +--------+    |         +--------+
                             ^         |         |        |
                             |         +-------> |   OR   | ---> (p q + r) +
  r -------------------------+                   |        |      _ _
         +--------+                              +--------+      p q
  p ---> |  NOT   | ---+                             ^
         +--------+    |   +--------+                |
                       +---|        |                |
                           |  AND   |----------------+
                       +---|        |
         +--------+    |   +--------+
  q ---> |  NOT   | ---+
         +--------+

Here, the longest path is: ** 'AND'; 'OR'; 'OR' **, which is clearly better
(even though the 'NOT' gate is quite short in comparison to the others).
                   
Example: Consider p + q + r + s.  There are four inputs.  How shall we lay
out the 'OR' gates so that the circuit settles as rapidly as possible?
Since 'OR' gates take time to settle, we should construct the shallowest
tree possible.  Always, this is the most balanced tree possible.  This will
minimize the longest path from a leaf (variable or subcircuit) to the root
(circuit output).  On the left, the worst tree.  On the right, the best tree.

          +                +         ^
         / \              / \        |
        p   +            /   \       |
           / \          +     +      | time
          q   +        / \   / \     |
             / \      p   q r   s    |
            r   s                    |

Exercise: What is the best layout for

  # pqr |= =| pq + pr + qr

?

De Morgan's Laws
________________

~(p /\ q) |= =| (~p) \/ (~q)

~(p \/ q) |= =| (~p) /\ (~q)


Computer Arithmetic
___________________

We start with addition and subtraction of binary integers.

Humans add binary numbers by acting as bit-serial adders.  With a bit of
practice, this becomes fairly automatic.

We can mathematically describe this process as repeatedly computing two
3-place Boolean functions, where the three arguments are the two bits 'p'
and 'q' plus the carry-in bit 'r = c_in'.  The two Boolean functions are
's' (sum) and 'c' (carry out).  We can specify both functions by means of
a table.

p  q  r  |  s  c
_________|______
0  0  0  |  0  0  s = (+)^3 pqr.
0  0  1  |  1  0  c = #pqr.
0  1  0  |  1  0
0  1  1  |  0  1
1  0  0  |  1  0
1  0  1  |  0  1
1  1  0  |  0  1
1  1  1  |  1  1

Note: Efficient humans do _not_ add by doing table lookup.

Example: 000111 =  7  (carries not shown)
       + 001101 = 13
         ------   --
         010100 = 20

To subtract, we compute a negation and then add.

Example: 000111 =  7  (carries not shown)
       + 111010 = -6
         ------   --
       1|000001 =  1

The interpretation of bit patterns as decimal numbers obviously depends on
the choice of semantics (natural number or two's complement), but the
manipulation of bit patterns does not.  There is one addition table above,
not one table per semantics.

Computers do addition using registers.  We can pretend our two examples take
place in 6-bit registers.  In the addition, there is no carry out from the
result register.  In the subtraction, there _is_ a carry out from the result
register, but the bit pattern in the result register is entirely correct.

By definition, _overflow_ occurs when the result of an operation cannot be
contained in the available hardware, in this case, a 6-bit register.  When
adding operands of opposite signs, overflow cannot occur.

The relationship between carry out and overflow is much simpler in 
natural-number semantics.  Here, an n-bit register can store any natural
number strictly less than 2^n.  Therefore, if the true result of an addition
is at least 2^n, then overflow has occurred.  Moreover, in such a case, the
addition algorithm will have produced a carry out.

Example: 111111 = 63  (carries not shown)
       + 111111 = 63
         ------   --
       1|111110 = 62  (correct answer: 126)

Here, we have both carry out and overflow.

Two's-complement semantics is slightly trickier.

What is the largest two's-complement integer that fits into a 6-bit register?
Answer: 2^5 - 1 = 31.  Therefore, we should be able to produce overflow by
adding 16 to itself.

Example: 010000 =  16  (carries not shown)
       + 010000 =  16
         ------
         100000 = -32

Even though there was no carry out from the result register, the correct
answer (viz., 32) cannot fit into a 6-bit register---if we are using
two's-complement semantics to interpret bit patterns.  In natural-number
semantics, of course, a 6-bit register can hold any natural number up to
63.

Returning to two's-complement semantics, we must say that overflow has
occurred.  The absence of a 7th bit means that overflow has occurred because
the sign bit has been set with the _value_ of the result, rather than with
the proper sign of the result.  We have overwritten the sign bit.

Hence, overflow occurs when adding two positive numbers and the sum is
negative, or vice versa.  This means a carry out has occurred into the sign
bit.

A computer can easily have two separate add instructions, one suitable for
general addition, with the hardware detecting and signaling overflow, and
another suitable for memory-address calculation, with the hardware neither
detecting nor signaling overflow.  Memory addresses are natural numbers,
so the rare overflow problems can be made the programmer's responsibility,
saving a bit of work by the hardware.

Thus, that horrible-looking instruction

daddiu r1,r1,#-8

means "add integer -8 (a 16-bit signed _immediate_) to register 'r1', but
ignore overflow".  The initial 'd' is another story, of no particular
interest.  But there aren't two _addition algorithms_, only the two options
of either paying attention to overflow or ignoring it, plus the obvious two
options of how to interpret the resulting bit pattern as an integer.  Since
-8 is negative, it would be sign extended to give a 32-bit signed integer
in two's complement.  The addition would take place.  But the sum, in the
case of a memory-address calculation, would be interpreted with 
natural-number semantics.  Again, programmer caution is advised.

Arithmetic in 16-bit registers can be shown with four hex digits.

Example: 002c  (+44)
        +ffff  ( -1)
         ----   ---
         002b  (+43)

Multiplication
______________

Multiplication is a bit trickier.  There isn't one way to do it.  The
simplest to explain corresponds to what we learned in lower school (assume
positive numbers):

- put multiplier in 32-bit register
- put multiplicand in 64-bit register
- initialize 64-bit product to zero

loop: test lsb of multiplier
      if 1, add multiplicand to product
      shift multiplicand register 1-bit left
      shift multiplier register 1-bit right
      if not done, goto loop

Example: 1101  (+13)  multiplicand
       x 1011  (+11)  multiplier
         ----
         1101
        1101
       0000
      1101
     --------
     10001111  (+143)  final result (product)

The simplest---but inefficient---hardware algorithm puts the multiplicand
in a 64-bit register that is shifted left as each new result is added in.
The sum is accumumulated in a 64-bit product register.  The multiplier
goes in a 32-bit register that is shifted right to serially highlight each
binary digit of the multiplier from right to left.

When the rightmost digit of the (currently shifted) multiplier is 1, we add
the shifted multiplicand to the product (accumulator register).  After this
operation, we shift the multiplicand left and the multiplier right.

Now, the easiest way to multiply two signed numbers is to convert both
operands to positive numbers, do traditional multiplication, and remember
the signs.

A diagram may help:

          +--------------------+            We are multiplying two 32-bit
          |         |          |            numbers, but we are using
          |    multiplicand    |  64 bits   several 64-bit registers.
          |         |          |
          +--------------------+
                    |
     +--------+     |
     |        |     |
     |    +--------------------+
     |    |                    |
     |    |       adder        |  64 bits
     |    |                    |
     |    +--------------------+
     |              |
     |    +--------------------+            +--------------+
     |    |                    |            |              |
     |    |      product       |   <test>   |  multiplier  |  32 bits
     |    |                    |            |              |
     |    +--------------------+            +--------------+
     |              |
     +--------------+

The shifting is automatic.  The test is primarily to determine if the shifted
multiplicand is to be added to the product.

Overflow will occur if the (final) product is too big to fit into 32 bits.

We will not cover integer division or floating-point arithmetic.

Synthesis Exercises I
_____________________

Definition: A set of connectives is _complete_ if they suffice to express
an arbitrary Bollean function.

'(+)^3' is ternary addition modulo 2.  '(+)^3 pqr' is true iff an odd number
of 'p', 'q', and 'r', is true.

Exercise: Show that {'(+)^3', '/\', 'T', 'F'} is complete.  (No proper subset
is complete).

Exercise: Develop an argument to show that {'(+)^3', '~', 'T', 'F'} is not
complete.

Exercise: Show {'|'} is complete.  Proof:     ~p |= =| p | p.
                                          p \/ q |= =| (~p) | (~q)

Since {'~', '\/'} is complete, and since both connectives can be simulated
using only '|', {'|'} is complete.  QED

Synthesis Exercises II
______________________

Logicians, and circuit designers, are concerned with 'completeness'.  As
defined earlier, a set of connectives is called _complete_ if an arbitrary
Boolean function can be computed using only members of this set.

Well-known examples of complete sets are {'~', '/\'} and {'~', '\/'}.  The
synthesis game started by taking sets of connectives and asking if they were
complete, but may be played without reference to completeness.

1. Show that {'~', '-->'} is complete.
   p --> q |= =| ~p \/ q
  ~p --> q |= =|  p \/ q

2. Show that {'|'} is complete.
   ~p |= =| p | p
   p \/ q |= =| ~p | ~q

3. 'I' is the ternary 'odd' connective.  'Ipqr' is true iff an odd
   number of its arguments is true.  Show that {'I', '/\', 'T', 'F'}
   is complete.
   ~p |= =| IpTF

4. 'P' is the ternary 'even' connective.  'Ppqr' is true iff an even
   number of its arguments is true.  Synthesize 'P' from {'~', '<-->'}.
   Ppqr |= =| ~Ipqr
        |= =| ~[(p + q) + r]
        |= =| (p + q) <--> r
        |= =| ~(p <--> q) <--> r

5. '2!' is the ternary 'precisely two' connective.  '2!pqr' is true
   iff precisely two of its arguments are true.  Show that {'2!', 'T'}
   is complete.
   ~p |= =| 2!pTT
   p /\ q |= =| 2!pq(~T)

6. 'M' is the ternary 'minority' connective.  'Mpqr' is true iff a
   minority of its arguments is true.  Show that {'M', 'F'} is
   complete.
   ~p |= =| MppF
   p /\ q |= =| ~MpqF (#pqF)


From probst at cse.concordia.ca  Sun Aug 16 16:29:43 2020
From: probst at cse.concordia.ca (David K. Probst)
Date: Sun, 16 Aug 2020 16:29:43 -0400
Subject: [comp5201-f20] assignment 2
Message-ID: <5f399737./mDmbrCxAYWhoO/4%probst@cse.concordia.ca>


Name: ___________________                                     ID: ____________

COMP5201                       Assignment 2                          Fall 2020
Issued: September 29, 2020                               Due: October 20, 2020
Submit electronically.  No extension will be granted.

1. [24 marks] Digital Logic.

a) 'X' is the ternary connective such that 'Xpqr' is logically equivalent to
'p /\ (p + q + r)'. '+' is 'xor'.  'F' and 'T' denote the 0-ary connectives
'false' and true, respectively.  Whenever possible, put letters in
alphabetical order, and put letters before any 0-place connectives.

Using {'X', 'F', 'T'}, synthesize:                  ~p |= =| X ____ ____ ____

Using {'X', 'T'}, synthesize:                   p /\ q |= =| X ____ ____ ____

Using {'X', '~', 'T'}, synthesize:         p \/ q |= =| ____ X ____ ____ ____

b) 'Y' is the ternary connective such that 'Ypqr' is logically equivalent to
'p /\ (q <--> r)'.  'p <--> q' is true iff 'p' and 'q' have the same truth
value.  'F' and 'T' denote the two 0-ary connectives 'false' and 'true',
respectively.  Whenever possible, put letters in alphabetical order, and put
letters before any 0-place connectives.

Using {'Y', 'F', 'T'}, synthesize:                  ~p |= =| Y ____ ____ ____

Using {'Y', 'F', 'T'}, synthesize:              p /\ q |= =| Y ____ ____ ____

Using {'Y', '~', 'F'}, synthesize:          p \/ q |= =| ___ Y ____ ____ ____

2. [16 marks] Binary and Hexadecimal Numbers.

Convert each of the following six binary or hexadecimal natural numbers into
decimal.  Show work.  For the last number, show either scientific notation
with six decimal places, or the exact integer.

a) Binary numbers: 1111, 1111 1110, 1111 1110 1101 0111

b) Hexadecimal numbers: af, afba, afba 51de

3. [25 marks] Fractional Numbers and Blackboard Notation.

Infinite binary expansions of rational numbers are either pure recurring or
mixed recurring depending on whether the cycle starts immediately after the
point.

a) [math] Show the infinite binary expansion of 6 4/9 without normalization.

b) [math] Show this infinite binary expansion in hexadecimal without
normalization.

c) [math] Show the infinite binary expansion of 6 4/9 with normalization.  Do
not forget the scale factor.  (This is a _scaled_ infinite binary expansion).

d) [math] Show this infinite binary expansion in hexadecimal.  Again, do not
forget the scale factor, which may be shown in decimal.

e) Show the normalized (binary) blackboard notation that best approximates
6 4/9.  The fractional field is 16 bits.  Show all 16 bits.  Now, show just
the 16-bit (4-hexit) fractional field in hexadecimal.

4. [15 marks] Integer Multiplication I.

a) Multiply the following 10-bit binary natural numbers.  The multiplicand
is 10011 11100 (27c hex) and the multiplier is 11010 (1a hex).   Show, in
hexadecimal, i) the initial value of the accumulator, and ii) each term
added to the accumulator, and the partial sum after the addition.  The last
addition yields the final value.

b) Redo the multiplication steps exactly as in question 4 a), but initialize
the accumulator to s = 11011 (1b hex) instead of 0.  Show the same
intermediate and final values.  (This is called "fused multiply-add"). 

5. [20 marks] Integer Multiplication II.

a) Show that, regardless of the initial n-bit value of the accumulator, the
fused multiply-add result of two n-bit natural-number operands is always
representable in 2n bits.  Now, suppose n = 16.  Starting from the largest
possible FMA result, what is the hexadecimal representation of the largest
n = 16-bit number that can _still_ be added without producing overflow?

b) A modular-adder device 'M' operates with 16-bit registers.  You give it
two 16-bit natural numbers 'a' and 'b'.  It adds them, divides by 2^16,
keeps the quotient 'q' a secret, and publishes the remainder 'r'.  Hint:
Before answering, experiment with small addition tables.

i)  If a = 31,465 and r = 53,576, what are 'b' and 'q'?

ii) If a = 35,492 and r = 11,087, what are 'b' and 'q'?


From probst at cse.concordia.ca  Sun Aug 30 16:18:15 2020
From: probst at cse.concordia.ca (David K. Probst)
Date: Sun, 30 Aug 2020 16:18:15 -0400
Subject: [comp5201-f20] lecture 5
Message-ID: <5f4c0987.2gjCNyzjXOv3wbUf%probst@cse.concordia.ca>


Lecture 5  Computer Organization and Design                    DKP/Fall 2020
_________

It is not immediately obvious that Moore's law should favor one type of
processor architecture over another.  Why, starting in 1980, did killer
micros have such success?  Which processor architectures died off?  What
is a killer micro, anyway?  Does _every_ microprocessor count as a killer
micro?

The Cray vector architectures died off because they were too expensive,
and also because parallel vector processors (i.e., computers) ran into
trouble scaling to larger number of processors.  But parallel vector
processors were supercomputers, pioneering advanced architectural ideas
with exotic technologies.  More down to earth, DEC's Vax architecture ran
into trouble because its complex instruction set made it very hard to
implement fast pipelining.  (The Vax can stand for other machines that
lost their appeal).  In 1980, the idea of RISC (reduced instruction set
computing) architecture gained ground, and led to radical simplifications
in the implementation of pipelining.

In the 1970s, machines had hundreds of different instructions, in a variety
of formats, leading to thousands of distinct combinations when
addressing-mode variations were taken into account.  Interpretation of all
possible combinations of opcodes and operands required very complex control
circuitry.  Early VLSI chips simply did not have enough room to hold both
this massive control circuitry and a register file with sufficiently many
registers.  RISC machines were successful because they radically reduced
the amount of control circuitry, i.e., they reduced the control _overhead_.
Moreover, the simplicity of control in RISC machines allowed a fully
hardwired implementation of the control circuitry, breaking with the
tradition of "software" microprogrammed control.  This dramatically
increased execution speed.

The distinction between RISC and CISC has not stayed hard and fast since
1980, especially after the introduction of Intel chips in the 1990s that
were CISC on the outside, and RISC on the inside.  Judged by the number of
chips sold, we have to count more recent Intel and AMD processors as killer
micros.  This is so even if they have processor architectures only a mother
could love.  Another concern is that CISC processors are incredibly
expensive and time-consuming to design.  You almost have to admire Intel
for not being killed by the complexity of its own designs.  Whether this
is a good starting point for multicore, or, indeed, for any _system on a
chip_, is another question entirely.  Finally, describing CISC architectures
to students is not exactly enjoyable.

We can sketch some of the core ideas in RISC design philosophy, which is
both an approach to instruction-set design, and a set of implementation
strategies.

1. There shall be a small set of instructions, each of which can be executed
in approximately the same amount of time using hardwired control (you may
need several RISC instructions to do the work of one complex instruction).

2. The architecture shall be a _load/store_ architecture that confines
memory-address calculation, and memory-latency delays, to a small set of
load and store instructions, with all other (register-register) instructions
obtaining their operands from faster, and compactly addressable, processor
registers.

3. There shall be a limited number of simple addressing modes that eliminate
or speed up address calculations for the vast majority of cases.

4. There shall be simple, uniform instruction formats that facilitate
extraction/decoding of the various fields.  This allows overlap between
opcode interpretation and register readout.

Admittedly, what ultimately counts are the run times of our programs.  What
matters is not whether the computer executes 10 billion simple instructions,
or 5 billion complex instructions, but simply what the total run time is.

We can characterize RISC architectures in slightly different language
(redundancy is not bad):

1. All operations on data apply to data in registers.

2. The only operators that affect memory are loads (which move data from
memory to a register) and stores (which move data from a register to memory).

3. The instruction formats are few in number, with all instructions typically
being one size.

RISC architectures include MIPS, ARM, PowerPC, PA-RISC, SPARC, and the
now-defunct Alpha.  However, if MIPS and ARM own the embedded market, the
most powerful computers today appear to be hybrids of Intel chips and 
Nvidia chips.  Why this is so is a long story.

Terminology
___________

Early descriptions of processor microarchitecture emphasized the distinction
between (active) control circuitry and (passive) datapath circuitry that just
followed orders.  As time went on, many control functions were incorporated
into the datapath.  Even so, the conceptual need for distinct control circuitry
remained because while we can imagine an intelligent datapath that can analyze
and control aspects of executing individual instructions, only separate control
circuitry can maintain a more global picture of how different instructions
should interact.

Moreover, all datapaths soon became pipelined.  Today, it is more natural to
speak of an instruction-execution pipeline with some control functionality that
makes use of various on-chip hardware resources, and is ultimately guided by
control circuitry that is _notionally_ outside the pipeline.

RISC pipelines were immediately successful, yet they did not stand still.
Rather, they continued to evolve as heavier and heavier performance demands
were placed on them.  This continued until the first RISC inflection point in
the 1990s.  In fact, the RISC microachitecture dominant in the 21st century is
radically different from the one dominant in the 20th century (the multilevel
cache was the troublemaker).  RISC 1.0 has in-order instruction-execution
pipelines: machine instructions are executed in _program order_.  RISC 2.0
has out-of-order instruction-execution pipelines: machine instructions are
executed _out of program order_ using dataflow ideas to overcome the strict
von Neumann control flow in RISC 1.0.  We start with the simpler RISC 1.0.  

RISC Instruction Execution
__________________________

We now study the MIPS RISC instruction-execution pipeline (at least in the
form that was popular in the 1980s).  We waste no time on nonpipelined
datapaths.  First,we need the concept of _pipeline_.

Consider a computer system that takes in operations on the left, computes
them, and then pushes out results on the right.  In a pipeline, we may push
in new operations on the left long before getting the results of previous
operations pushed out on the right.  A pipeline is characterized by three
parameters: First, there is the peak input bandwidth (the maximum input rate
the pipeline will tolerate).  Second, there is the operation latency (the
time for an operation to complete).  Third, there is the pipeline occupancy
(the number of uncompleted operations in the pipeline at any one time).

Pipelines take time to reach their equilibrium state.  We may feed operations
into an empty pipeline, but need to wait until we get our first result.  It
is only after the pipeline has reached its equilbrium state that the result
bandwidth on the right will exactly match the input bandwidth on the left.
Here is a picture:
                          pipeline

                     +-----------------+
                -->  |  -->  -->  -->  |  -->
bandwidth = 2        | concurrency = 6 |        bandwidth = 2
                -->  |  -->  -->  -->  |  -->
                     +-----------------+
 
                       latency = 3

Suppose initially the pipeline is empty, and we apply a sustained input
bandwidth of two operations per cycle.  Eventually, the pipeline will reach
equlilibrium, and we will receive a a sustained output of two results per
cycle.  In the picture, it looks like we are supplying peak input bandwidth
but I never said this.  In general (Little's Law), at equilibrium, the
occupancy (concurrency) of the pipeline is the latency-bandwidth product.

Pipelining has performance consequences.  In the picture above, system
throughput (result bandwidth) is 2 results per cycle.  In contrast, if we
were to wait for the previous result(s) before supplying new input,
throughput would drop to 2/3 result per cycle.  In general, pipelining a
system multiplies the throughput by the latency.  

The MIPS 'fdxmw' instruction-execution pipeline is a special case of the
general pipeline sketched above.  At equilibrium, the input bandwidth is 1,
the output bandwidth is 1, the latency is 5, and the occupancy (concurrency)
is 5.   

The MIPS pipeline is divided into five stages (or "boxes"), each of which
performs a well-defined task.  Here is a picture:

+-------------------------------------------------------------------------+ P
|     <I-cache>               <Register file>                 <D-cache>   | r
|                                                                         | o
|                             <Control circuitry>                         | c
|              +-+           +-+           +-+           +-+              | e
|              | |           | |           | |           | |              | s
|     <f-box>  | |  <d-box>  | |  <x-box>  | |  <m-box>  | |  <w-box>     | s
|              +-+           +-+           +-+           +-+              | o
|              f/d           d/x           x/m           m/w              | r
+-------------------------------------------------------------------------+

In addition to the square boxes (shown using angular brackets and box names),
which you may think of as combinational logic (this is a slight exaggeration),
there are rectangular boxes, called "latches", which you may think of as
sequential logic (actually, they are merely collections of state elements).
There is a flow of data, from left to right, through boxes and latches.  Less
happily, there is bidirectional data flow, at great cost in time and energy,
between certain pipeline boxes and the hardware resources pictured at the top
of the processor.  Remember the time and energy required to move data!

1. The f-box reads the memory address of the next instruction from the PC
register inside the f/d latch, and fetches it from memory.  It then updates
PC by adding 4.  Each box must complete its work, including all latching, in
one clock cycle.  The I-cache intercepts the memory request and, if it has a
copy of the instruction, delivers the instruction to the f-box.  Thus, 32 bits
of address travel up to the I-cache, and 32 bits of instruction travel down to
the f-box.  Before the cycle completes, the f-box latches the fetched
instruction in the f/d latch.  Note that the f-box pokes the I-cache with a
memory byte address, which is nothing other than an _array index_; after all,
memory is an array of bytes.

2. The d-box has multiple tasks.  It must decode the instruction, noting the
operands (register and immediate), and the destination register (if any).  It
also localizes any register operands in the instruction from the register file.
Register names travel up to the register file, and register values travel down
to the d-box, which latches them in the d/x latch.  Note that the d-box pokes
the register file with one or more register names, which are _array indices_,
if we view the register file as an array of registers.  No instruction has
room to store register values; the registor fields in an instruction contain
register designators.

The d-box also processes conditional branches.  Suppose the branch is
'bne r1,r2,loop'.  The d-box tests inequality of the two registers it has
localized.  If they are not equal, the branch is deemed taken, and the d-box
adds the offset (a multiple of the immediate 'loop') to the base register PC,
thus causing the branch to take place on the next successful fetch.  With
16-bit immediates, we can jump 2^15 instructions up or down.

Again, all these tasks must be completed within a single clock cycle.  In
particular, the branch-target address must be written into PC before the cycle
completes, if the branch is taken.  Many of the names/values isolated during
decoding are passed down the pipeline as data for use by other boxes.

3. The x-box is actually a case statement, which obviously requires control.

i) In a memory reference, we add the base register and the offset to compute
the memory address.

ii) In a register-register instruction, we perform the operation specified 
by the opcode.

iii) In a register-immediate instruction, we perform the operation specified
by the opcode, using the immediate as an operand value.

Do x-boxes exist?  Of course, there are no such things as ALUs; instead, there
are various functional units, perhaps an integer adder, an integer multiplier,
a floating-point adder, a floating-point multiplier, a shifter, a
logic-operation unit, etc., etc.  So the x-box begins to seem somewhat ghostly
if you think about it too carefully.

Also, although the x-box is a case statement, it is not really the one we have
described.  Consider 'l.d f6,-24(r2)', 'add r1,r2,r3', and 'addi r1,r1,8'.
As far as the x-box is concerned, these are all _one case_.  The x-box's
job here is simply to perform an integer addition; where these integers may
have come from is irrelevant.  In contrast, 'mul.d f0,f2,f4' is a different
case, since now the x-box needs to perform a floating-point multiplication.

4. The m-box is another case statement.  If the instruction is a load, the
m-box reads from memory, and latches the result in the m/w latch.  If the
instruction is a store, the m-box writes to memory taking the value to be
stored from some pipeline latch.  Otherwise, the m-box does nothing.  Of
course, data and control must sometimes flow from the x-box to the w-box
even if the m-box itself does nothing.  This is called "bypassing".  Note
that the D-cache stands between the m-box and the real memory.  The D-cache
also intercepts memory requests. 

5. The w-box does the same thing if the instruction is a load, or an ALU
instruction, which necessarily produces a result.  Namely, it takes the
loaded value, or the ALU result, which must both be in some pipeline
register, and writes it in the destination register in the register file.

Not all instructions use all five boxes.  A conditional branch uses only the
f-box and the d-box.  A store uses f, d, x, and m.  An ALU instruction uses
f, d, x, and w.  Only a load uses f, d, x, m, and w.

Information Flow
________________

Consider the x-, m-, and w-boxes.  Here is a brief description of the data
flow.  If the x-box is to act on some value or values, it must receive it
(or them).  Also, it may receive a further data value that specifies more
precisely the action to be performed (e.g., the number of bits to shift).
In a store instruction, the m-box receives a data value from the pipeline,
namely, the value to be stored.  It also receives a data value that is the
memory address of where in memory the former value is to be stored.  In a
load instruction, the m-box only receives the memory address.  When the
instruction contains a destination register, the w-box receives both a data
value, and a register name from the pipeline.  This is what is to be written
back and where it is to be written back to.  Obviously, one only feeds data
to a box when one intends that box to do something.

The control signals are too many to permit a brief description.  We therefore
invent a set of toy control signals that give us some of the flavor of control,
 without getting us bogged down in unnecessary details.  The toy control
signals are 'no-op', 'add', 'multiply', 'shift', 'load', 'store', and 'write
back'.  If any box receives a box-appropriate control signal at the start of
cycle 'c', then it will perform the specified action during cycle 'c'.  The
'no-op' signal means "do nothing".

Reality Check
_____________

Consider the f-box.  Allegedly, it sends PC to memory and fetches the next
instruction.  Although we don't call this a _memory reference_, i.e., a load
or a store, it certainly is a _memory access_.  How many processor cycles
does it take to access memory on a typical computer?  In lecture 1, I
suggested the figure of 200 cycles.  You cannot address memory and get back
a word in a single cycle.  What is going on here?

In reality, the f-box sends PC to the _instruction cache, or _I-cache_.
A cache is a complex state element that can store copies of some, but
obviously not all, memory locations.  Most caches (L1$, L2$, L3$) are small
enough to fit on the processor chip.  So, we have to pretend two things that
are not true.  First, that it is possible to access the I-cache in a single
cycle.  Second, that the I-cache magically _always_ has a copy of just the
instruction we need.  Surprisingly, for reasons explained later, these two
white lies do not grossly misrepresent the actual performance of the I-cache.
Hint: this good fortune has something to do with the fetch-execute cycle.

Consider the m-box.  Allegedly,	it either sends a data address to memory and
receives a data value (this is a load), or else it sends a data address plus
a data value to memory, thus depositing the data value (this is a store).
Recall that the m-box only does work when the pipeline is executing a memory
reference.  Still, each memory reference is certainly a memory access.  Do
we have a problem here?

In reality, the m-box interacts with the _data cache_, or _D-cache_.  This is
another complex state element that can store copies of some, but obviously not
all, memory locations.  Again, we pretend two things that are not true.  First,
that it is possible to access the D-cache in a single cycle.  Second, that the
D-cache magically _always_ has a copy of the data value we wish to load or
store.  For data, these two white lies _can_ cause serious problems.  It
depends on the memory-accessing pattern.

Consider program P executing during some interval I.  Suppose that, during this
interval, program P only uses data from, say, 20K distinct memory locations.
We say that these 20K locations constitute the _working set_ of program P
during interval I.  If program P's working set fits into the computer's
D-cache, then we can't really say we have a problem.  However, if the working
set doesn't fit into the D-cache, then we _may_ have a problem: we may
repeatedly fail to find the data copy we want in the D-cache, and thus may
repeatedly be forced to access the actual memory, at much greater cost.
Which of these two cases is the real one depends on the memory-accessing
pattern of program P.  Some patterns generate enormous working sets that are
too big to fit into any D-cache.

This problem even has a name: it is officially called the _Memory Wall_.
Consider Moore's Law.  It says that processor performance, which we may
roughly model as the product of the number of logic transistors times the
clock frequency in Hz, increases exponentially over time.  Before 2003,
both factors increased exponentially.  Now, we have to be very careful with
the clock frequency.  Still, replacing a single power-inefficient core with
many power-efficient cores increases the power efficiency of the processor
chip as a whole, and thereby allows us to increase performance without
overstepping our power budget.  We don't ask that each factor in Moore's Law
increases exponentially; we only ask that their _product_ increases
exponentially.  Moore's Law, interpreted in this fashion, is alive and well
in the multicore era, with one important qualification.  Update: In 2020,
Moore's Law is far from being alive and well.

Increased arithmetic performance is only possible if there is increased
delivery of data operands to functional units.  Memories are making some
improvements to memory latency and memory bandwidth, but not fast enough to
keep pace with processor improvements.  Raw processor performance increases
faster than raw memory performance.  A ideal, or perfect, cache could easily
compensate for the mismatch between processor demand and memory supply.
However, in the real world, caches are _not_ ideal, and programs are not
always _cache friendly_.  In the worst case, program performance may be
limited by memory performance, and be unaffected by increases in processor
performance.  All moderately educated computer professionals agree that the
Memory Wall and the Power Wall are the two main challenges that must be
overcome by today's computer designers.  Dealing with the Power Wall is
mandatory.  What about the Memory Wall?  We could accept to only write
programs that play nicely with today's caches.  But this concession might
be selling our birthright for a mess of pottage.

Pipelining
__________

Pipelining is a naturally efficient way for a datapath to execute machine
instructions.  It was standard in vector supercomputers.  The basic idea is
to allow different workstations ("boxes") to work on different instructions
at the same time.  By overlapping the execution of different machine 
instructions, we can make significant improvements in the pipeline's execution
_throughput_.

Say that executing some machine instruction is a task 'T'.  Suppose we break 
'T' into a _sequence_ of nonoverlapping subtasks: 'T = t1; t2; ...; tn'.  Now,
we provide a specialized workstation for each subtask (these are our boxes).
Think of the processor clock as ringing a bell at the start of each processor
cycle.  For simplicity, let's stick with having five stages, with the f, d,
x, m, and w boxes as our specialized workstations.  An instruction pipeline
doesn't have to have five stages---it could have 21---but this was the design
of an early RISC processor.

When the bell rings, each workstation passes the partially executed instruction
to the workstation on its right.

A picture may help:

         1  2  3  4  5  6  7  8  9  0  ...  clock cycle
instr 1  f  d  x  m  w
instr 2     f  d  x  m  w
instr 3        f  d  x  m  w
instr 4           f  d  x  m  w
instr 5              f  d  x  m  w
instr 6                 f  d  x  m  w
instr 7                    f  d  x  m  w
...                           ...

Look at clock cycle 5.  We have five boxes working on five different
instructions.

Is this advantageous?  Suppose each box completes its work in one clock cycle.
Therefore, the time to execute an instruction (the instruction _latency_) is
5 cycles.  But when the pipeline achieves cruising speed, a new instruction
completes every cycle, giving the pipeline an execution _bandwidth_ of one
instruction per cycle.  (We call this _single-cycle pipelining_).  The ideal
speedup when a pipeline is pipelined is thus equal to the pipeline _depth_
(the number of stages).

Each of our five boxes is a _combinational_ circuit, but the clocked pipeline
as a whole is a _sequential_ circuit.  To make this work, each box is followed 
immediately on its right by a set of (nonISA) _pipeline registers_, which is
called a "pipeline latch".  The basic requirement is this: Prior to the end of
a clock cycle, all the results from a given stage must be stored in the
pipeline latch to its right, in order that these values can be preserved
across clock cycles, and used as inputs to the next stage at the start of the
next clock cycle.

Also, it is reasonable to think of the boxes as combinational circuits that
compute Boolean functions, and of the latches as finite state machines that
provide control of boxes---among other things.

Exercise: Pick a cycle and a box and describe the control plus data received
at the beginning of that cycle.

Space-time Diagram
__________________

The figure above is a space-time diagram.  These diagrams show the evolution 
of the pipeline in time, and are useful in capturing aspects of the control
and data flow.  Let me repeat the figure with some actual (but not very
interesting) sets of instructions.

               1  2  3  4  5  6  7  8  9
add r1,r2,r3   f  d  x  m  w
add r2,r3,r4      f  d  x  m  w
add r3,r4,r5         f  d  x  m  w
add r4,r5,r6            f  d  x  m  w
add r5,r6,r7               f  d  x  m  w

The instruction-execution pipeline reaches equilibrium in cycle 5, when all 
five boxes are (notionally) active, and five distinct instructions are being
processed at the same time.  At equilibrium, one instruction is fetched per
cycle, one result is produced per cycle, the latency is 5 cycles, and the
pipeline occupancy (concurrency) is 5, which is the latency-bandwidth product.

In this toy program, no data flows between instructions, which are all
independent.  As described earlier, data does flow between different boxes
working on the same instruction.  For example, in the first instruction, the
x-box receives the values of 'r2' and 'r3', which it adds.  The w-box receives
the register name "r1" and the sum computed by the x-box.  However, the m-box
receives no data at all.  Since an add is not a memory reference, there is no
work from the m-box to do.  Instead, it performs a no-op.  I sometimes write
"n" in place of "m" when I want to emphasize that the m-box is inactive.  The
data sent to the w-box hops right over the m-box.

In this diagram, what control signals do the boxes receive?  Consider cycle 1.
The f-box receives the control signal 'fetch', while _all_ the other boxes
receive the control signal 'no-op'.  By reading the columns of the diagram,
one can see which boxes receive activating control signals, and which boxes
are instructed to do nothing.  (Qualification follows).

The exception to this rule is the m-box.  Although I have written "m" in five
columns, in fact, the m-box receives the 'no-op' control signal in _each and
every_ cycle (not just the columns in which "m" appears).

In every cycle, the f- and d-boxes receive either the 'no-op' control signal
or the 'fetch' and 'decode' control signals, respectively.  From a control
standpoint, they are not very interesting case statements.  For this reason,
we focus on the x-, m-, and w-boxes.

When the x-box does something, it may be any one of a range of arithmetic or
logical operations.  When the m-box does something, it is either a load or a
store.  When the w-box does something, it is always write back.

A pipeline is a machine with a control system.  We will see more interesting
space-time diagrams when we learn about pipeline dynamics.  But a space-time
diagram by itself shows the pipeline's kinematics.  For all boxes other than
the m-box, if a box name appears in the column corresponding to cycle 'c',
then that box is active in cycle 'c'.  Howver, if the m-box's name appears,
then it may or may not be active.

